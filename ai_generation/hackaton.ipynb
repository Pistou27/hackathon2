{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9db95d3",
   "metadata": {},
   "source": [
    "Welcome to your Hackathon !\n",
    "\n",
    "Hackathon\n",
    "\n",
    "\n",
    "Idea 1: Generative AI Content Pipeline with Workflow Automation\n",
    "\n",
    "\n",
    "Objective:\n",
    "\n",
    "Build an end-to-end automated pipeline that (1) generates text or images, (2) checks content quality, (3) applies ethical filtering, and (4) runs on modest CPU resources.\n",
    "\n",
    "\n",
    "Step-by-Step Instructions\n",
    "\n",
    "1. Define the Task & Pipeline Overview\n",
    "\n",
    "    First, decide whether you’ll generate articles, summaries, or images.\n",
    "    Then, sketch a high-level workflow:\n",
    "\n",
    "\n",
    "Prompt → Generation Model → Quality-Check Module → (Optional) Image Generator → Ethical Filter → Output\n",
    "\n",
    "\n",
    "2. Select Your Generation Method\n",
    "\n",
    "    First, choose your text generation model family (e.g. Transformer-based) or image model (e.g. VAE/GAN).\n",
    "    Then, justify why that family fits your goal (speed, quality, CPU constraints).\n",
    "\n",
    "\n",
    "3. Pick Specific Pre-trained Models\n",
    "\n",
    "    First, for text: pick a small model like distilGPT2 or t5-small.\n",
    "    Then, for summarization: pick distilBERT or BART-base.\n",
    "    Optionally, for images: choose a simple VAE (avoid heavy GANs on CPU).\n",
    "\n",
    "\n",
    "4. Prepare & Subsample Your Dataset\n",
    "\n",
    "    First, select a dataset (e.g. IMDB reviews for text, CIFAR-10 for images).\n",
    "    Then, subsample to a manageable size (e.g. 5% of the full dataset).\n",
    "\n",
    "\n",
    "5. Implement Text Generation Module\n",
    "\n",
    "    First, write code to load your pre-trained LLM (e.g. via Hugging Face’s transformers).\n",
    "    Then, create a function that takes a prompt and returns generated text.\n",
    "\n",
    "\n",
    "6. Add Summarization for Quality Control\n",
    "\n",
    "    First, load your BERT-based summarizer.\n",
    "    Then, feed generated text into the summarizer and compare key points to the prompt to catch off-topic or low-quality outputs.\n",
    "\n",
    "\n",
    "7. (Optional) Implement Image/Text Generation with VAE\n",
    "\n",
    "    First, construct your VAE architecture for small images.\n",
    "    Then, train or load a pre-trained VAE on your subsampled dataset.\n",
    "\n",
    "\n",
    "8. Automate the Workflow\n",
    "\n",
    "    First, choose an automation tool or scheduler (e.g. cron jobs, Airflow, or a simple Python script with schedule).\n",
    "    Then, write a script that runs your generation + quality check + (optional) image step at fixed intervals or on demand.\n",
    "\n",
    "\n",
    "9. Evaluate Your System\n",
    "\n",
    "    First, generate a test set of 50–100 samples.\n",
    "    Then, compute metrics:\n",
    "        Text: BLEU, ROUGE, perplexity\n",
    "        Robustness: adversarial prompts or noise injection\n",
    "\n",
    "\n",
    "10. Integrate an Ethical Filter\n",
    "\n",
    "    First, decide what to flag (e.g. bias, hate speech, misinformation).\n",
    "    Then, plug in a simple classifier or rule-based detector after generation to filter or tag problematic outputs.\n",
    "\n",
    "\n",
    "11. Document & Reflect\n",
    "\n",
    "    First, write a one-page summary of your pipeline architecture and why you chose each component.\n",
    "    Then, reflect on CPU trade-offs, quality outcomes, and ethical challenges.\n",
    "\n",
    "\n",
    "What will you use?\n",
    "Module \tConcepts / Libraries / Models\n",
    "Generation \tTransformers; distilGPT2, t5-small; transformers library\n",
    "Summarization QC \tBERT; distilBERT or BART-base; sentence-transformers\n",
    "Image Module (optional) \tVAE; GAN (only on very small data); PyTorch or TensorFlow\n",
    "Data Handling \tdatasets library; subsampling techniques\n",
    "Automation \tcron; Python schedule; Apache Airflow or similar\n",
    "Evaluation Metrics \tBLEU; ROUGE; perplexity; adversarial testing\n",
    "Ethical Filtering \tBias detection; rule-based filters; simple classifier models\n",
    "Optimization Tips \tsmall batch sizes; CPU-friendly model variants; distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c0085",
   "metadata": {},
   "source": [
    "PROMPT UTILISATEUR\n",
    "    ↓\n",
    "distilGPT2 (génération de texte)\n",
    "    ↓\n",
    "BART (résumé) → comparaison au prompt (cohérence)\n",
    "    ↓\n",
    "Filtre éthique (règles simples + classifieur si possible)\n",
    "    ↓\n",
    "Texte final + évaluation qualité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f219ea4",
   "metadata": {},
   "source": [
    "Famille choisie : Transformers\n",
    "Critère\tJustification\n",
    "Vitesse\tdistilGPT2 et distilBART sont rapides sur CPU\n",
    "Qualité\tBonne cohérence contextuelle et diversité des sorties\n",
    "Contraintes CPU\tModèles distillés (moins de poids, chargement rapide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eeb56a",
   "metadata": {},
   "source": [
    "Génération\tdistilgpt2\tLéger, bon sur CPU, cohérent\n",
    "Résumé / QC\tsshleifer/distilbart-cnn-12-6\tRésumé rapide et précis\n",
    "Filtrage éthique\tDétecteur naïf + classifieur\tÀ construire avec regex + score (option SVM)\n",
    "Tous les modèles seront chargés via Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b1a33",
   "metadata": {},
   "source": [
    "Dataset : IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209d469",
   "metadata": {},
   "source": [
    "Texte libre, nombreux avis structurés\n",
    "\n",
    "Utilisé comme base pour prompts ou pour tester le pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b536af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mathi\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 531691.81 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 601772.19 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 610949.69 examples/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:5%]\")  # ~1250 exemples\n",
    "dataset = dataset.shuffle(seed=42).select(range(50))  # on en garde 50 pour test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef21742",
   "metadata": {},
   "source": [
    "On entre maintenant dans la phase d’exécution, à partir de l’Étape 5 : Text Generation que j’ai déjà partiellement codée plus haut, mais cette fois on la formalisera dans le rapport du hackathon avec tout le contexte, le code, les choix techniques et les tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506aaf0e",
   "metadata": {},
   "source": [
    "Créer une fonction qui prend un prompt utilisateur, le passe à un modèle pré-entraîné (distilGPT2) et retourne un texte généré.\n",
    "Choix technique :\n",
    "\n",
    "    Modèle : distilgpt2\n",
    "\n",
    "    Librairie : Hugging Face Transformers\n",
    "\n",
    "    Périphérique : CPU uniquement\n",
    "\n",
    "    Paramètres de génération : top_k, top_p, temperature pour plus de diversité\n",
    "\n",
    "    Format : Fonction Python réutilisable dans un pipeline\n",
    "\n",
    "Fonctionnement du module :\n",
    "\n",
    "    Charger le tokenizer et le modèle distilGPT2\n",
    "\n",
    "    Encodage du prompt en input IDs\n",
    "\n",
    "    Génération avec sampling contrôlé\n",
    "\n",
    "    Décodage en texte lisible\n",
    "\n",
    "Pourquoi distilGPT2 ?\n",
    "\n",
    "    Version distillée de GPT-2\n",
    "\n",
    "    Compatible CPU\n",
    "\n",
    "    Temps de génération faible (< 1s pour 80 tokens)\n",
    "\n",
    "    Qualité suffisante pour des tests de workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727d2e3",
   "metadata": {},
   "source": [
    "Résumé automatique pour contrôle qualité\n",
    "1. Rapport pédagogique – Étape 6\n",
    "Objectif\n",
    "\n",
    "Vérifier que le texte généré est pertinent par rapport au prompt d’origine.\n",
    "On applique un résumé automatique au texte généré, puis on compare le résumé au prompt :\n",
    "\n",
    "    Si le résumé ne contient aucun lien avec le prompt, le texte est probablement hors sujet.\n",
    "\n",
    "    On pourra aussi calculer des similarités (étape suivante).\n",
    "\n",
    "Choix du modèle\n",
    "\n",
    "    Modèle : sshleifer/distilbart-cnn-12-6\n",
    "\n",
    "    Pourquoi :\n",
    "\n",
    "        Version distillée de BART (CPU-friendly)\n",
    "\n",
    "        Entraîné sur tâche de résumé CNN/DailyMail\n",
    "\n",
    "        Compatible Hugging Face Transformers\n",
    "\n",
    "        Moins lourd que facebook/bart-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fcfb0",
   "metadata": {},
   "source": [
    "Évaluation rapide\n",
    "Critère\tRésultat\n",
    "Chargement modèle\t~10 sec (CPU)\n",
    "Qualité du résumé\tBonne condensation d’idées\n",
    "Compatibilité CPU\tOk\n",
    "Fiabilité comme QC\tMoyenne seule — à coupler ensuite\n",
    "\n",
    "Le résumé seul ne suffit pas à juger si le texte est correct → on ajoutera une comparaison par similarité dans l'étape suivante.\n",
    "\n",
    "Ajouter une vérification automatique du type :\n",
    "\n",
    "Compare(PROMPT, SUMMARY) → score de similarité → seuil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb0df9",
   "metadata": {},
   "source": [
    "Vérification automatique de la qualité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1cd743",
   "metadata": {},
   "source": [
    "Nous allons comparer le prompt initial et le résumé du texte généré à l’aide d’une similarité sémantique basée sur Sentence Transformers et cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e7e74f",
   "metadata": {},
   "source": [
    "Rapport pédagogique – Contrôle de cohérence\n",
    "Objectif :\n",
    "\n",
    "Évaluer automatiquement si le texte généré reste proche du prompt. On passe le résumé du texte généré et le prompt à un encodeur de phrase, puis on mesure leur similarité.\n",
    "Outil utilisé :\n",
    "\n",
    "    sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "        Modèle léger pour embeddings de phrases\n",
    "\n",
    "        Très rapide sur CPU\n",
    "\n",
    "        Bon équilibre vitesse/précision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610528e",
   "metadata": {},
   "source": [
    "≥ 0.70\tBonne cohérence thématique\n",
    "0.50 – 0.70\tCohérence moyenne / partielle\n",
    "< 0.50\tRésumé souvent hors-sujet ou trop vague"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0724f6",
   "metadata": {},
   "source": [
    "VAE (Variational Autoencoder) sur Images\n",
    "Rapport pédagogique – Génération d’images par VAE\n",
    "Objectif :\n",
    "\n",
    "Compléter le pipeline par une génération d’images simples à partir de données visuelles comprimées, typiquement des chiffres manuscrits (MNIST).\n",
    "Ce module est séparé du pipeline texte, mais montre la capacité à intégrer un autre générateur CPU-friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e2f0e",
   "metadata": {},
   "source": [
    "Pourquoi VAE ?\n",
    "Critère\tAvantage\n",
    "Léger et efficace\tFonctionne bien sur CPU\n",
    "Interprétable\tApprentissage probabiliste + compression des images\n",
    "Idéal sur MNIST\tFormat 28x28, rapide à entraîner/tester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5417da3",
   "metadata": {},
   "source": [
    "Génération d’une image manuscrite aléatoire.\n",
    "\n",
    "Exécutable même sur CPU sans CUDA.\n",
    "\n",
    "Montre comment on peut élargir le pipeline IA à d’autres modalités."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9bd41",
   "metadata": {},
   "source": [
    "RÉCAPITULATIF DU PIPELINE – Hackathon IA Générative CPU-Friendly\n",
    "Objectif global\n",
    "\n",
    "Créer un pipeline de génération de contenu automatique, avec contrôle qualité et filtrage éthique, 100% CPU compatible, en respectant une structure modulaire, claire et réutilisable.\n",
    "Modules implémentés\n",
    "Étape\tNom du module\tDescription\n",
    "1\tmain.py\tOrchestration du pipeline\n",
    "2\tdata_loader.py\tChargement et sous-échantillonnage du dataset IMDB (5% puis 50 exemples)\n",
    "3\tgeneration.py\tGénération de texte avec distilGPT2\n",
    "4\tsummarization.py\tRésumé automatique avec distilBART-cnn-12-6\n",
    "5\tsimilarity.py\tVérification de cohérence par similarité (prompt vs résumé)\n",
    "6\tvae_model.py\tVAE simple pour images (MNIST, 28x28)\n",
    "7\ttrain_vae.py\tEntraînement du VAE sur CPU\n",
    "8\tgenerate_image.py\tGénération aléatoire d’image depuis l’espace latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebd325",
   "metadata": {},
   "source": [
    "PROMPT UTILISATEUR\n",
    "    ↓\n",
    "distilGPT2 (génération)\n",
    "    ↓\n",
    "distilBART (résumé du texte généré)\n",
    "    ↓\n",
    "all-MiniLM (similarité résumé ↔ prompt)\n",
    "    ↓\n",
    "(score ∈ [0, 1]) → QC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07224a3f",
   "metadata": {},
   "source": [
    "Espace latent Z aléatoire\n",
    "    ↓\n",
    "Décodeur VAE (pré-entraîné)\n",
    "    ↓\n",
    "Image MNIST générée (28x28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b765b0",
   "metadata": {},
   "source": [
    "Génération de texte fonctionnelle\n",
    "\n",
    "Résumé de qualité correcte\n",
    "\n",
    "Score de similarité utilisable pour QC\n",
    "\n",
    "VAE image testé avec succès\n",
    "\n",
    "Tout est CPU-friendly, exécuté localement, bien commenté"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9aa282",
   "metadata": {},
   "source": [
    "Automate le Pipeline\n",
    "Rapport pédagogique\n",
    "Objectif :\n",
    "\n",
    "Permettre une exécution complète du pipeline texte (de la génération à la vérification de cohérence), en un seul appel de script, pour :\n",
    "\n",
    "    Tester rapidement plusieurs exemples\n",
    "\n",
    "    Permettre une future exécution automatisée\n",
    "\n",
    "    Garder des logs de résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce971b3f",
   "metadata": {},
   "source": [
    "python automate.py\tExécute immédiatement 3 exemples du pipeline texte\n",
    "python automate_schedule.py\tPlanifie le pipeline toutes les X minutes (ajustable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed09f2e",
   "metadata": {},
   "source": [
    "Évaluer le système\n",
    "Rapport pédagogique\n",
    "Objectif :\n",
    "\n",
    "Évaluer la qualité du texte généré par le pipeline, et la robustesse du système face à des prompts tordus ou bruités.\n",
    "Métriques utilisées\n",
    "Type\tMétrique\tUtilité\n",
    "Qualité texte\tBLEU\tCorrespondance n-grammes entre référence/généré\n",
    "\tROUGE-L\tSimilarité sur séquences longues (résumés)\n",
    "Cohérence interne\tSimilarité cosinus\tDéjà utilisée pour QC via embeddings\n",
    "Robustesse\tAdversarial prompts\tPrompts perturbés pour tester stabilité\n",
    "Mise en place d’un benchmark\n",
    "\n",
    "On va :\n",
    "\n",
    "    Générer 10 exemples (avec prompts IMDB)\n",
    "\n",
    "    Pour chacun :\n",
    "\n",
    "        Comparer prompt vs résumé (notre référence)\n",
    "\n",
    "        Calculer BLEU et ROUGE-L\n",
    "\n",
    "    Évaluer sur quelques prompts adversariaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13fbd47",
   "metadata": {},
   "source": [
    "Filtrage éthique du contenu généré"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c92ee",
   "metadata": {},
   "source": [
    "Rapport pédagogique\n",
    "Objectif :\n",
    "\n",
    "Créer un filtre post-génération qui :\n",
    "\n",
    "    Tague ou rejette les textes non conformes\n",
    "\n",
    "    Se base sur des critères simples mais extensibles\n",
    "\n",
    "    Fonctionne en local, sans API commerciale\n",
    "\n",
    "Deux approches combinées :\n",
    "Méthode\tAvantage\tLimite\n",
    "Règles simples (regex/keywords)\tUltra rapide, contrôlable\tFaille sur langage détourné\n",
    "Classifieur léger (SVM ou NB)\tDétection plus souple (ton, thème)\tEntraînement minimal requis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ae33d",
   "metadata": {},
   "source": [
    "# ✅ Étape 11 – Synthèse finale et réflexion\n",
    "\n",
    "## 🎯 Objectifs atteints\n",
    "\n",
    "| Objectif                                         | Statut     |\n",
    "|--------------------------------------------------|------------|\n",
    "| Génération de texte stable et rapide             | ✅ Réussi  |\n",
    "| Résumé automatique pour contrôle                 | ✅ Réussi  |\n",
    "| Comparaison résumé/prompt (similarité)           | ✅ Réussi  |\n",
    "| Évaluation BLEU/ROUGE                            | ✅ Intégré |\n",
    "| Génération image CPU avec VAE                    | ✅ Optionnel, fait |\n",
    "| Filtrage éthique intégré                         | ✅ Par règles |\n",
    "| Orchestration automatisée                        | ✅ CLI & Schedule |\n",
    "| Modularité / testabilité                         | ✅ Bonne   |\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 Apports pédagogiques\n",
    "\n",
    "- Apprentissage de la **structure d’un pipeline NLP complet**\n",
    "- Pratique du **multi-modèle Hugging Face**\n",
    "- Maîtrise des **métriques d’évaluation** (BLEU, ROUGE)\n",
    "- Introduction au **filtrage éthique local**\n",
    "- Compréhension des **limites sur CPU**\n",
    "- Réflexion sur la **cohérence générative automatique**\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ Limites et pistes d'amélioration\n",
    "\n",
    "| Limite                               | Suggestion d’amélioration               |\n",
    "|--------------------------------------|-----------------------------------------|\n",
    "| Filtre éthique basé sur mots-clés    | Ajouter un SVM ou BERT toxique local    |\n",
    "| Prompt limité à 100 tokens           | Adapter dynamiquement selon modèle      |\n",
    "| Aucun feedback utilisateur           | Ajouter une interface Streamlit         |\n",
    "| Évaluation humaine absente           | Créer un tableau de scoring qualitatif  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
