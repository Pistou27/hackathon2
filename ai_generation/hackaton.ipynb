{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9db95d3",
   "metadata": {},
   "source": [
    "Welcome to your Hackathon !\n",
    "\n",
    "Hackathon\n",
    "\n",
    "\n",
    "Idea 1: Generative AI Content Pipeline with Workflow Automation\n",
    "\n",
    "\n",
    "Objective:\n",
    "\n",
    "Build an end-to-end automated pipeline that (1) generates text or images, (2) checks content quality, (3) applies ethical filtering, and (4) runs on modest CPU resources.\n",
    "\n",
    "\n",
    "Step-by-Step Instructions\n",
    "\n",
    "1. Define the Task & Pipeline Overview\n",
    "\n",
    "    First, decide whether you‚Äôll generate articles, summaries, or images.\n",
    "    Then, sketch a high-level workflow:\n",
    "\n",
    "\n",
    "Prompt ‚Üí Generation Model ‚Üí Quality-Check Module ‚Üí (Optional) Image Generator ‚Üí Ethical Filter ‚Üí Output\n",
    "\n",
    "\n",
    "2. Select Your Generation Method\n",
    "\n",
    "    First, choose your text generation model family (e.g. Transformer-based) or image model (e.g. VAE/GAN).\n",
    "    Then, justify why that family fits your goal (speed, quality, CPU constraints).\n",
    "\n",
    "\n",
    "3. Pick Specific Pre-trained Models\n",
    "\n",
    "    First, for text: pick a small model like distilGPT2 or t5-small.\n",
    "    Then, for summarization: pick distilBERT or BART-base.\n",
    "    Optionally, for images: choose a simple VAE (avoid heavy GANs on CPU).\n",
    "\n",
    "\n",
    "4. Prepare & Subsample Your Dataset\n",
    "\n",
    "    First, select a dataset (e.g. IMDB reviews for text, CIFAR-10 for images).\n",
    "    Then, subsample to a manageable size (e.g. 5% of the full dataset).\n",
    "\n",
    "\n",
    "5. Implement Text Generation Module\n",
    "\n",
    "    First, write code to load your pre-trained LLM (e.g. via Hugging Face‚Äôs transformers).\n",
    "    Then, create a function that takes a prompt and returns generated text.\n",
    "\n",
    "\n",
    "6. Add Summarization for Quality Control\n",
    "\n",
    "    First, load your BERT-based summarizer.\n",
    "    Then, feed generated text into the summarizer and compare key points to the prompt to catch off-topic or low-quality outputs.\n",
    "\n",
    "\n",
    "7. (Optional) Implement Image/Text Generation with VAE\n",
    "\n",
    "    First, construct your VAE architecture for small images.\n",
    "    Then, train or load a pre-trained VAE on your subsampled dataset.\n",
    "\n",
    "\n",
    "8. Automate the Workflow\n",
    "\n",
    "    First, choose an automation tool or scheduler (e.g. cron jobs, Airflow, or a simple Python script with schedule).\n",
    "    Then, write a script that runs your generation + quality check + (optional) image step at fixed intervals or on demand.\n",
    "\n",
    "\n",
    "9. Evaluate Your System\n",
    "\n",
    "    First, generate a test set of 50‚Äì100 samples.\n",
    "    Then, compute metrics:\n",
    "        Text: BLEU, ROUGE, perplexity\n",
    "        Robustness: adversarial prompts or noise injection\n",
    "\n",
    "\n",
    "10. Integrate an Ethical Filter\n",
    "\n",
    "    First, decide what to flag (e.g. bias, hate speech, misinformation).\n",
    "    Then, plug in a simple classifier or rule-based detector after generation to filter or tag problematic outputs.\n",
    "\n",
    "\n",
    "11. Document & Reflect\n",
    "\n",
    "    First, write a one-page summary of your pipeline architecture and why you chose each component.\n",
    "    Then, reflect on CPU trade-offs, quality outcomes, and ethical challenges.\n",
    "\n",
    "\n",
    "What will you use?\n",
    "Module \tConcepts / Libraries / Models\n",
    "Generation \tTransformers; distilGPT2, t5-small; transformers library\n",
    "Summarization QC \tBERT; distilBERT or BART-base; sentence-transformers\n",
    "Image Module (optional) \tVAE; GAN (only on very small data); PyTorch or TensorFlow\n",
    "Data Handling \tdatasets library; subsampling techniques\n",
    "Automation \tcron; Python schedule; Apache Airflow or similar\n",
    "Evaluation Metrics \tBLEU; ROUGE; perplexity; adversarial testing\n",
    "Ethical Filtering \tBias detection; rule-based filters; simple classifier models\n",
    "Optimization Tips \tsmall batch sizes; CPU-friendly model variants; distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c0085",
   "metadata": {},
   "source": [
    "PROMPT UTILISATEUR\n",
    "    ‚Üì\n",
    "distilGPT2 (g√©n√©ration de texte)\n",
    "    ‚Üì\n",
    "BART (r√©sum√©) ‚Üí comparaison au prompt (coh√©rence)\n",
    "    ‚Üì\n",
    "Filtre √©thique (r√®gles simples + classifieur si possible)\n",
    "    ‚Üì\n",
    "Texte final + √©valuation qualit√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f219ea4",
   "metadata": {},
   "source": [
    "Famille choisie : Transformers\n",
    "Crit√®re\tJustification\n",
    "Vitesse\tdistilGPT2 et distilBART sont rapides sur CPU\n",
    "Qualit√©\tBonne coh√©rence contextuelle et diversit√© des sorties\n",
    "Contraintes CPU\tMod√®les distill√©s (moins de poids, chargement rapide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eeb56a",
   "metadata": {},
   "source": [
    "G√©n√©ration\tdistilgpt2\tL√©ger, bon sur CPU, coh√©rent\n",
    "R√©sum√© / QC\tsshleifer/distilbart-cnn-12-6\tR√©sum√© rapide et pr√©cis\n",
    "Filtrage √©thique\tD√©tecteur na√Øf + classifieur\t√Ä construire avec regex + score (option SVM)\n",
    "Tous les mod√®les seront charg√©s via Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b1a33",
   "metadata": {},
   "source": [
    "Dataset : IMDB Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209d469",
   "metadata": {},
   "source": [
    "Texte libre, nombreux avis structur√©s\n",
    "\n",
    "Utilis√© comme base pour prompts ou pour tester le pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b536af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mathi\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 531691.81 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 601772.19 examples/s]\n",
      "Generating unsupervised split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:00<00:00, 610949.69 examples/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:5%]\")  # ~1250 exemples\n",
    "dataset = dataset.shuffle(seed=42).select(range(50))  # on en garde 50 pour test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef21742",
   "metadata": {},
   "source": [
    "On entre maintenant dans la phase d‚Äôex√©cution, √† partir de l‚Äô√âtape 5 : Text Generation que j‚Äôai d√©j√† partiellement cod√©e plus haut, mais cette fois on la formalisera dans le rapport du hackathon avec tout le contexte, le code, les choix techniques et les tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506aaf0e",
   "metadata": {},
   "source": [
    "Cr√©er une fonction qui prend un prompt utilisateur, le passe √† un mod√®le pr√©-entra√Æn√© (distilGPT2) et retourne un texte g√©n√©r√©.\n",
    "Choix technique :\n",
    "\n",
    "    Mod√®le : distilgpt2\n",
    "\n",
    "    Librairie : Hugging Face Transformers\n",
    "\n",
    "    P√©riph√©rique : CPU uniquement\n",
    "\n",
    "    Param√®tres de g√©n√©ration : top_k, top_p, temperature pour plus de diversit√©\n",
    "\n",
    "    Format : Fonction Python r√©utilisable dans un pipeline\n",
    "\n",
    "Fonctionnement du module :\n",
    "\n",
    "    Charger le tokenizer et le mod√®le distilGPT2\n",
    "\n",
    "    Encodage du prompt en input IDs\n",
    "\n",
    "    G√©n√©ration avec sampling contr√¥l√©\n",
    "\n",
    "    D√©codage en texte lisible\n",
    "\n",
    "Pourquoi distilGPT2 ?\n",
    "\n",
    "    Version distill√©e de GPT-2\n",
    "\n",
    "    Compatible CPU\n",
    "\n",
    "    Temps de g√©n√©ration faible (< 1s pour 80 tokens)\n",
    "\n",
    "    Qualit√© suffisante pour des tests de workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727d2e3",
   "metadata": {},
   "source": [
    "R√©sum√© automatique pour contr√¥le qualit√©\n",
    "1. Rapport p√©dagogique ‚Äì √âtape 6\n",
    "Objectif\n",
    "\n",
    "V√©rifier que le texte g√©n√©r√© est pertinent par rapport au prompt d‚Äôorigine.\n",
    "On applique un r√©sum√© automatique au texte g√©n√©r√©, puis on compare le r√©sum√© au prompt :\n",
    "\n",
    "    Si le r√©sum√© ne contient aucun lien avec le prompt, le texte est probablement hors sujet.\n",
    "\n",
    "    On pourra aussi calculer des similarit√©s (√©tape suivante).\n",
    "\n",
    "Choix du mod√®le\n",
    "\n",
    "    Mod√®le : sshleifer/distilbart-cnn-12-6\n",
    "\n",
    "    Pourquoi :\n",
    "\n",
    "        Version distill√©e de BART (CPU-friendly)\n",
    "\n",
    "        Entra√Æn√© sur t√¢che de r√©sum√© CNN/DailyMail\n",
    "\n",
    "        Compatible Hugging Face Transformers\n",
    "\n",
    "        Moins lourd que facebook/bart-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fcfb0",
   "metadata": {},
   "source": [
    "√âvaluation rapide\n",
    "Crit√®re\tR√©sultat\n",
    "Chargement mod√®le\t~10 sec (CPU)\n",
    "Qualit√© du r√©sum√©\tBonne condensation d‚Äôid√©es\n",
    "Compatibilit√© CPU\tOk\n",
    "Fiabilit√© comme QC\tMoyenne seule ‚Äî √† coupler ensuite\n",
    "\n",
    "Le r√©sum√© seul ne suffit pas √† juger si le texte est correct ‚Üí on ajoutera une comparaison par similarit√© dans l'√©tape suivante.\n",
    "\n",
    "Ajouter une v√©rification automatique du type :\n",
    "\n",
    "Compare(PROMPT, SUMMARY) ‚Üí score de similarit√© ‚Üí seuil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb0df9",
   "metadata": {},
   "source": [
    "V√©rification automatique de la qualit√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1cd743",
   "metadata": {},
   "source": [
    "Nous allons comparer le prompt initial et le r√©sum√© du texte g√©n√©r√© √† l‚Äôaide d‚Äôune similarit√© s√©mantique bas√©e sur Sentence Transformers et cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e7e74f",
   "metadata": {},
   "source": [
    "Rapport p√©dagogique ‚Äì Contr√¥le de coh√©rence\n",
    "Objectif :\n",
    "\n",
    "√âvaluer automatiquement si le texte g√©n√©r√© reste proche du prompt. On passe le r√©sum√© du texte g√©n√©r√© et le prompt √† un encodeur de phrase, puis on mesure leur similarit√©.\n",
    "Outil utilis√© :\n",
    "\n",
    "    sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "        Mod√®le l√©ger pour embeddings de phrases\n",
    "\n",
    "        Tr√®s rapide sur CPU\n",
    "\n",
    "        Bon √©quilibre vitesse/pr√©cision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610528e",
   "metadata": {},
   "source": [
    "‚â• 0.70\tBonne coh√©rence th√©matique\n",
    "0.50 ‚Äì 0.70\tCoh√©rence moyenne / partielle\n",
    "< 0.50\tR√©sum√© souvent hors-sujet ou trop vague"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0724f6",
   "metadata": {},
   "source": [
    "VAE (Variational Autoencoder) sur Images\n",
    "Rapport p√©dagogique ‚Äì G√©n√©ration d‚Äôimages par VAE\n",
    "Objectif :\n",
    "\n",
    "Compl√©ter le pipeline par une g√©n√©ration d‚Äôimages simples √† partir de donn√©es visuelles comprim√©es, typiquement des chiffres manuscrits (MNIST).\n",
    "Ce module est s√©par√© du pipeline texte, mais montre la capacit√© √† int√©grer un autre g√©n√©rateur CPU-friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e2f0e",
   "metadata": {},
   "source": [
    "Pourquoi VAE ?\n",
    "Crit√®re\tAvantage\n",
    "L√©ger et efficace\tFonctionne bien sur CPU\n",
    "Interpr√©table\tApprentissage probabiliste + compression des images\n",
    "Id√©al sur MNIST\tFormat 28x28, rapide √† entra√Æner/tester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5417da3",
   "metadata": {},
   "source": [
    "G√©n√©ration d‚Äôune image manuscrite al√©atoire.\n",
    "\n",
    "Ex√©cutable m√™me sur CPU sans CUDA.\n",
    "\n",
    "Montre comment on peut √©largir le pipeline IA √† d‚Äôautres modalit√©s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9bd41",
   "metadata": {},
   "source": [
    "R√âCAPITULATIF DU PIPELINE ‚Äì Hackathon IA G√©n√©rative CPU-Friendly\n",
    "Objectif global\n",
    "\n",
    "Cr√©er un pipeline de g√©n√©ration de contenu automatique, avec contr√¥le qualit√© et filtrage √©thique, 100% CPU compatible, en respectant une structure modulaire, claire et r√©utilisable.\n",
    "Modules impl√©ment√©s\n",
    "√âtape\tNom du module\tDescription\n",
    "1\tmain.py\tOrchestration du pipeline\n",
    "2\tdata_loader.py\tChargement et sous-√©chantillonnage du dataset IMDB (5% puis 50 exemples)\n",
    "3\tgeneration.py\tG√©n√©ration de texte avec distilGPT2\n",
    "4\tsummarization.py\tR√©sum√© automatique avec distilBART-cnn-12-6\n",
    "5\tsimilarity.py\tV√©rification de coh√©rence par similarit√© (prompt vs r√©sum√©)\n",
    "6\tvae_model.py\tVAE simple pour images (MNIST, 28x28)\n",
    "7\ttrain_vae.py\tEntra√Ænement du VAE sur CPU\n",
    "8\tgenerate_image.py\tG√©n√©ration al√©atoire d‚Äôimage depuis l‚Äôespace latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebd325",
   "metadata": {},
   "source": [
    "PROMPT UTILISATEUR\n",
    "    ‚Üì\n",
    "distilGPT2 (g√©n√©ration)\n",
    "    ‚Üì\n",
    "distilBART (r√©sum√© du texte g√©n√©r√©)\n",
    "    ‚Üì\n",
    "all-MiniLM (similarit√© r√©sum√© ‚Üî prompt)\n",
    "    ‚Üì\n",
    "(score ‚àà [0, 1]) ‚Üí QC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07224a3f",
   "metadata": {},
   "source": [
    "Espace latent Z al√©atoire\n",
    "    ‚Üì\n",
    "D√©codeur VAE (pr√©-entra√Æn√©)\n",
    "    ‚Üì\n",
    "Image MNIST g√©n√©r√©e (28x28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b765b0",
   "metadata": {},
   "source": [
    "G√©n√©ration de texte fonctionnelle\n",
    "\n",
    "R√©sum√© de qualit√© correcte\n",
    "\n",
    "Score de similarit√© utilisable pour QC\n",
    "\n",
    "VAE image test√© avec succ√®s\n",
    "\n",
    "Tout est CPU-friendly, ex√©cut√© localement, bien comment√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9aa282",
   "metadata": {},
   "source": [
    "Automate le Pipeline\n",
    "Rapport p√©dagogique\n",
    "Objectif :\n",
    "\n",
    "Permettre une ex√©cution compl√®te du pipeline texte (de la g√©n√©ration √† la v√©rification de coh√©rence), en un seul appel de script, pour :\n",
    "\n",
    "    Tester rapidement plusieurs exemples\n",
    "\n",
    "    Permettre une future ex√©cution automatis√©e\n",
    "\n",
    "    Garder des logs de r√©sultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce971b3f",
   "metadata": {},
   "source": [
    "python automate.py\tEx√©cute imm√©diatement 3 exemples du pipeline texte\n",
    "python automate_schedule.py\tPlanifie le pipeline toutes les X minutes (ajustable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed09f2e",
   "metadata": {},
   "source": [
    "√âvaluer le syst√®me\n",
    "Rapport p√©dagogique\n",
    "Objectif :\n",
    "\n",
    "√âvaluer la qualit√© du texte g√©n√©r√© par le pipeline, et la robustesse du syst√®me face √† des prompts tordus ou bruit√©s.\n",
    "M√©triques utilis√©es\n",
    "Type\tM√©trique\tUtilit√©\n",
    "Qualit√© texte\tBLEU\tCorrespondance n-grammes entre r√©f√©rence/g√©n√©r√©\n",
    "\tROUGE-L\tSimilarit√© sur s√©quences longues (r√©sum√©s)\n",
    "Coh√©rence interne\tSimilarit√© cosinus\tD√©j√† utilis√©e pour QC via embeddings\n",
    "Robustesse\tAdversarial prompts\tPrompts perturb√©s pour tester stabilit√©\n",
    "Mise en place d‚Äôun benchmark\n",
    "\n",
    "On va :\n",
    "\n",
    "    G√©n√©rer 10 exemples (avec prompts IMDB)\n",
    "\n",
    "    Pour chacun :\n",
    "\n",
    "        Comparer prompt vs r√©sum√© (notre r√©f√©rence)\n",
    "\n",
    "        Calculer BLEU et ROUGE-L\n",
    "\n",
    "    √âvaluer sur quelques prompts adversariaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13fbd47",
   "metadata": {},
   "source": [
    "Filtrage √©thique du contenu g√©n√©r√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c92ee",
   "metadata": {},
   "source": [
    "Rapport p√©dagogique\n",
    "Objectif :\n",
    "\n",
    "Cr√©er un filtre post-g√©n√©ration qui :\n",
    "\n",
    "    Tague ou rejette les textes non conformes\n",
    "\n",
    "    Se base sur des crit√®res simples mais extensibles\n",
    "\n",
    "    Fonctionne en local, sans API commerciale\n",
    "\n",
    "Deux approches combin√©es :\n",
    "M√©thode\tAvantage\tLimite\n",
    "R√®gles simples (regex/keywords)\tUltra rapide, contr√¥lable\tFaille sur langage d√©tourn√©\n",
    "Classifieur l√©ger (SVM ou NB)\tD√©tection plus souple (ton, th√®me)\tEntra√Ænement minimal requis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ae33d",
   "metadata": {},
   "source": [
    "# ‚úÖ √âtape 11 ‚Äì Synth√®se finale et r√©flexion\n",
    "\n",
    "## üéØ Objectifs atteints\n",
    "\n",
    "| Objectif                                         | Statut     |\n",
    "|--------------------------------------------------|------------|\n",
    "| G√©n√©ration de texte stable et rapide             | ‚úÖ R√©ussi  |\n",
    "| R√©sum√© automatique pour contr√¥le                 | ‚úÖ R√©ussi  |\n",
    "| Comparaison r√©sum√©/prompt (similarit√©)           | ‚úÖ R√©ussi  |\n",
    "| √âvaluation BLEU/ROUGE                            | ‚úÖ Int√©gr√© |\n",
    "| G√©n√©ration image CPU avec VAE                    | ‚úÖ Optionnel, fait |\n",
    "| Filtrage √©thique int√©gr√©                         | ‚úÖ Par r√®gles |\n",
    "| Orchestration automatis√©e                        | ‚úÖ CLI & Schedule |\n",
    "| Modularit√© / testabilit√©                         | ‚úÖ Bonne   |\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Apports p√©dagogiques\n",
    "\n",
    "- Apprentissage de la **structure d‚Äôun pipeline NLP complet**\n",
    "- Pratique du **multi-mod√®le Hugging Face**\n",
    "- Ma√Ætrise des **m√©triques d‚Äô√©valuation** (BLEU, ROUGE)\n",
    "- Introduction au **filtrage √©thique local**\n",
    "- Compr√©hension des **limites sur CPU**\n",
    "- R√©flexion sur la **coh√©rence g√©n√©rative automatique**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Limites et pistes d'am√©lioration\n",
    "\n",
    "| Limite                               | Suggestion d‚Äôam√©lioration               |\n",
    "|--------------------------------------|-----------------------------------------|\n",
    "| Filtre √©thique bas√© sur mots-cl√©s    | Ajouter un SVM ou BERT toxique local    |\n",
    "| Prompt limit√© √† 100 tokens           | Adapter dynamiquement selon mod√®le      |\n",
    "| Aucun feedback utilisateur           | Ajouter une interface Streamlit         |\n",
    "| √âvaluation humaine absente           | Cr√©er un tableau de scoring qualitatif  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
